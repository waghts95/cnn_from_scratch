{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 Assignment Roadmap: CNN Architecture Implementation\n",
        "\n",
        "This roadmap follows the official assignment README step by step.  \n",
        "Each part lists the requirements and our implementation progress.  \n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Part 1: CNN Layer Implementation (40 points)\n",
        "- [x] **Conv2D Layer** → forward + backward, stride & padding, He initialization  \n",
        "- [x] **MaxPool2D Layer** → forward + backward, gradient routing  \n",
        "- [x] **Flatten Layer**  \n",
        "- [x] **Dense Layer (Fully Connected)**  \n",
        "- [x] **ReLU Activation**  \n",
        "- [x] **Dropout / Dropout2D**  \n",
        "- [x] **BatchNorm2D**  \n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Part 2: CNN Architectures (30 points)\n",
        "- [x] **LeNet-5** → implemented with BatchNorm  \n",
        "- [x] **Mini-VGG** → implemented with BatchNorm + Dropout  \n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Part 3: CIFAR-10 Classification (20 points)\n",
        "\n",
        "### 3.1 Data Preprocessing\n",
        "- [x] Data normalization ([0,1] scaling)  \n",
        "- [x] Data augmentation (flip, rotation, shift)  \n",
        "- [ ] Cropping augmentation (explicit in README, but skipped here)  \n",
        "- [x] Train/validation/test split  \n",
        "\n",
        "### 3.2 Training\n",
        "- [x] Training loop with forward → loss → backward → update  \n",
        "- [x] SGD optimizer  \n",
        "- [x] Cross-entropy loss with softmax  \n",
        "- [x] Learning rate scheduling (step decay scheduler)  \n",
        "- [ ] Plot training/validation metrics (loss & accuracy curves)  \n",
        "\n",
        "### 3.3 Evaluation\n",
        "- [x] Accuracy calculation  \n",
        "- [x] Confusion matrix  \n",
        "- [x] Per-class accuracy  \n",
        "- [x] Confusion matrix visualization  \n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Part 4: Feature Visualization (10 points)\n",
        "- [x] Filter visualization (first Conv layer filters)  \n",
        "- [x] Feature map visualization (activations at chosen layers)  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "p96eC2igaPuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required Imports"
      ],
      "metadata": {
        "id": "CKTqbu49eOEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Required Imports\n",
        "# ==============================\n",
        "import numpy as np   # NumPy is used for array operations\n",
        "from typing import Tuple  # for type hints\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from scipy.ndimage import rotate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "bWZXBvmQdgH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: CNN Layer Implementation"
      ],
      "metadata": {
        "id": "0H0IWiZ2a0KA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hB5jpkB94NOt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ==============================\n",
        "# Base Layer Class\n",
        "# ==============================\n",
        "class Layer:\n",
        "    \"\"\"\n",
        "    Base class for all neural network layers.\n",
        "    Every layer must have forward() and backward() methods.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.trainable = True        # if the layer has weights (Conv, Dense)\n",
        "        self.params = {}             # store weights\n",
        "        self.grads = {}              # store gradients\n",
        "        self.cache = {}              # store intermediate values for backprop\n",
        "\n",
        "    def forward(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Conv2D Layer\n",
        "# ==============================\n",
        "class Conv2D(Layer):\n",
        "    \"\"\"\n",
        "    2D Convolution Layer\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    in_channels : int   → Number of input channels (e.g., 3 for RGB images)\n",
        "    out_channels : int  → Number of filters (output channels)\n",
        "    kernel_size : int   → Size of the convolution filter (e.g., 3 for 3x3)\n",
        "    stride : int        → How much the filter moves each step\n",
        "    padding : str       → 'same' (keep size same) or 'valid' (no padding)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int,\n",
        "                 kernel_size: int = 3, stride: int = 1,\n",
        "                 padding: str = 'same'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Save layer configuration\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = (kernel_size, kernel_size)  # assume square filters\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        # He initialization: good for ReLU networks\n",
        "        kh, kw = self.kernel_size\n",
        "        scale = np.sqrt(2.0 / (in_channels * kh * kw))\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        # W shape: (out_channels, in_channels, kernel_h, kernel_w)\n",
        "        self.params['W'] = np.random.randn(out_channels, in_channels, kh, kw) * scale\n",
        "        # Bias for each filter\n",
        "        self.params['b'] = np.zeros((out_channels, 1))\n",
        "\n",
        "        # Cache for storing intermediate values for backward\n",
        "        self.cache = {}\n",
        "\n",
        "    def _get_pad_width(self, input_shape: Tuple[int, ...]) -> Tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Calculate padding for 'same' or 'valid'.\n",
        "        \"\"\"\n",
        "        _, _, H, W = input_shape\n",
        "        kh, kw = self.kernel_size\n",
        "\n",
        "        if self.padding == 'same':\n",
        "            # Formula ensures output size ≈ input size\n",
        "            pad_h = max((np.ceil(H / self.stride) - 1) * self.stride + kh - H, 0)\n",
        "            pad_w = max((np.ceil(W / self.stride) - 1) * self.stride + kw - W, 0)\n",
        "            return int(pad_h // 2), int(pad_w // 2)\n",
        "        elif self.padding == 'valid':\n",
        "            return 0, 0\n",
        "        else:\n",
        "            raise ValueError(\"Padding must be 'same' or 'valid'\")\n",
        "\n",
        "    def _pad_input(self, x: np.ndarray, pad_h: int, pad_w: int) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Pad input with zeros around the border.\n",
        "        x shape: (batch, channels, height, width)\n",
        "        \"\"\"\n",
        "        return np.pad(x, ((0, 0), (0, 0), (pad_h, pad_h), (pad_w, pad_w)), mode='constant')\n",
        "\n",
        "    def forward(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Forward pass of convolution.\n",
        "        Input shape:  (batch, in_channels, H, W)\n",
        "        Output shape: (batch, out_channels, H_out, W_out)\n",
        "        \"\"\"\n",
        "        batch_size, _, H, W = x.shape\n",
        "        kh, kw = self.kernel_size\n",
        "        stride = self.stride\n",
        "        pad_h, pad_w = self._get_pad_width(x.shape)\n",
        "\n",
        "        # Pad input if necessary\n",
        "        x_padded = self._pad_input(x, pad_h, pad_w)\n",
        "\n",
        "        # Calculate output dimensions\n",
        "        H_out = (H + 2*pad_h - kh) // stride + 1\n",
        "        W_out = (W + 2*pad_w - kw) // stride + 1\n",
        "\n",
        "        # Allocate output tensor\n",
        "        out = np.zeros((batch_size, self.out_channels, H_out, W_out))\n",
        "\n",
        "        # Convolution operation\n",
        "        W = self.params['W']\n",
        "        b = self.params['b']\n",
        "\n",
        "        for n in range(batch_size):          # for each image\n",
        "            for f in range(self.out_channels):  # for each filter\n",
        "                for i in range(H_out):      # slide vertically\n",
        "                    for j in range(W_out):  # slide horizontally\n",
        "                        h_start = i * stride\n",
        "                        h_end = h_start + kh\n",
        "                        w_start = j * stride\n",
        "                        w_end = w_start + kw\n",
        "\n",
        "                        # Extract the patch of the image\n",
        "                        patch = x_padded[n, :, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                        # Convolution = sum(patch * filter) + bias\n",
        "                        out[n, f, i, j] = np.sum(patch * W[f]) + b[f]\n",
        "\n",
        "        # Save values for backward pass\n",
        "        self.cache = {\"x\": x, \"x_padded\": x_padded, \"pad_h\": pad_h, \"pad_w\": pad_w}\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass of convolution.\n",
        "        grad_output: gradient of loss w.r.t. layer output\n",
        "        Returns: gradient w.r.t. layer input\n",
        "        \"\"\"\n",
        "        x = self.cache['x']\n",
        "        x_padded = self.cache['x_padded']\n",
        "        pad_h, pad_w = self.cache['pad_h'], self.cache['pad_w']\n",
        "\n",
        "        batch_size, _, H, W = x.shape\n",
        "        kh, kw = self.kernel_size\n",
        "        stride = self.stride\n",
        "        _, _, H_out, W_out = grad_output.shape\n",
        "\n",
        "        # Initialize gradients\n",
        "        dW = np.zeros_like(self.params['W'])\n",
        "        db = np.zeros_like(self.params['b'])\n",
        "        dx_padded = np.zeros_like(x_padded)\n",
        "\n",
        "        # Compute gradients\n",
        "        for n in range(batch_size):\n",
        "            for f in range(self.out_channels):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        h_start = i * stride\n",
        "                        h_end = h_start + kh\n",
        "                        w_start = j * stride\n",
        "                        w_end = w_start + kw\n",
        "\n",
        "                        patch = x_padded[n, :, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                        # Gradients of weights and bias\n",
        "                        dW[f] += grad_output[n, f, i, j] * patch\n",
        "                        db[f] += grad_output[n, f, i, j]\n",
        "\n",
        "                        # Gradient wrt input\n",
        "                        dx_padded[n, :, h_start:h_end, w_start:w_end] += grad_output[n, f, i, j] * self.params['W'][f]\n",
        "\n",
        "        # Remove padding from dx\n",
        "        if pad_h > 0 or pad_w > 0:\n",
        "            dx = dx_padded[:, :, pad_h:-pad_h, pad_w:-pad_w]\n",
        "        else:\n",
        "            dx = dx_padded\n",
        "\n",
        "        # Save gradients\n",
        "        self.grads['W'] = dW\n",
        "        self.grads['b'] = db\n",
        "\n",
        "        return dx\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2D(Layer):\n",
        "    \"\"\"\n",
        "    2D Max Pooling Layer\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    pool_size : int or tuple\n",
        "        Size of pooling window (e.g., 2 → 2x2 pooling)\n",
        "    stride : int\n",
        "        Step size for moving the pooling window\n",
        "        If None, defaults to pool_size\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pool_size: int = 2, stride: int = None):\n",
        "        super().__init__()\n",
        "\n",
        "        # If given a single int, make it a square window\n",
        "        self.pool_size = (pool_size, pool_size) if isinstance(pool_size, int) else pool_size\n",
        "        self.stride = stride if stride is not None else pool_size\n",
        "\n",
        "        # Pooling has no trainable parameters\n",
        "        self.trainable = False\n",
        "        self.cache = {}\n",
        "\n",
        "    def forward(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Forward pass of max pooling.\n",
        "\n",
        "        Input shape:  (batch, channels, H, W)\n",
        "        Output shape: (batch, channels, H_out, W_out)\n",
        "        \"\"\"\n",
        "        batch_size, channels, H, W = x.shape\n",
        "        ph, pw = self.pool_size\n",
        "        stride = self.stride\n",
        "\n",
        "        # Calculate output dimensions\n",
        "        H_out = (H - ph) // stride + 1\n",
        "        W_out = (W - pw) // stride + 1\n",
        "\n",
        "        # Allocate output and mask (to remember max locations)\n",
        "        out = np.zeros((batch_size, channels, H_out, W_out))\n",
        "        max_indices = {}\n",
        "\n",
        "        for n in range(batch_size):\n",
        "            for c in range(channels):\n",
        "                for i in range(H_out):\n",
        "                    for j in range(W_out):\n",
        "                        h_start = i * stride\n",
        "                        h_end = h_start + ph\n",
        "                        w_start = j * stride\n",
        "                        w_end = w_start + pw\n",
        "\n",
        "                        # Extract pooling region\n",
        "                        region = x[n, c, h_start:h_end, w_start:w_end]\n",
        "\n",
        "                        # Find maximum value in region\n",
        "                        max_val = np.max(region)\n",
        "                        out[n, c, i, j] = max_val\n",
        "\n",
        "                        # Store index of max for backward\n",
        "                        max_pos = np.unravel_index(np.argmax(region), region.shape)\n",
        "                        max_indices[(n, c, i, j)] = (h_start + max_pos[0], w_start + max_pos[1])\n",
        "\n",
        "        # Save cache for backward pass\n",
        "        self.cache = {\"x_shape\": x.shape, \"max_indices\": max_indices,\n",
        "                      \"pool_size\": self.pool_size, \"stride\": stride}\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass of max pooling.\n",
        "\n",
        "        grad_output: gradient of loss w.r.t. pooled output\n",
        "        Returns: gradient w.r.t. input (same shape as forward input)\n",
        "        \"\"\"\n",
        "        x_shape = self.cache[\"x_shape\"]\n",
        "        max_indices = self.cache[\"max_indices\"]\n",
        "\n",
        "        # Initialize gradient w.r.t input with zeros\n",
        "        dx = np.zeros(x_shape)\n",
        "\n",
        "        for (n, c, i, j), (h_idx, w_idx) in max_indices.items():\n",
        "            # Route gradient only to the max location\n",
        "            dx[n, c, h_idx, w_idx] += grad_output[n, c, i, j]\n",
        "\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "MKYO5Ju49H2o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Flatten Layer\n",
        "# ==============================\n",
        "class Flatten(Layer):\n",
        "    \"\"\"\n",
        "    Flatten layer converts 4D input (batch, channels, H, W)\n",
        "    into 2D (batch, features).\n",
        "    Example:\n",
        "        Input shape:  (32, 16, 7, 7)\n",
        "        Output shape: (32, 16*7*7)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.trainable = False  # no weights\n",
        "\n",
        "    def forward(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        # Save input shape for backward\n",
        "        self.cache[\"input_shape\"] = x.shape\n",
        "        return x.reshape(x.shape[0], -1)\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        # Reshape gradient back to original input shape\n",
        "        return grad_output.reshape(self.cache[\"input_shape\"])\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Dense (Fully Connected) Layer\n",
        "# ==============================\n",
        "class Dense(Layer):\n",
        "    \"\"\"\n",
        "    Fully connected (linear) layer.\n",
        "    y = xW^T + b\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    in_features : int  → number of input neurons\n",
        "    out_features : int → number of output neurons\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # He initialization for weights\n",
        "        scale = np.sqrt(2.0 / in_features)\n",
        "        self.params[\"W\"] = np.random.randn(out_features, in_features) * scale\n",
        "        self.params[\"b\"] = np.zeros((out_features, 1))\n",
        "\n",
        "    def forward(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        Input:  (batch, in_features)\n",
        "        Output: (batch, out_features)\n",
        "        \"\"\"\n",
        "        self.cache[\"x\"] = x\n",
        "        return x.dot(self.params[\"W\"].T) + self.params[\"b\"].T\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass.\n",
        "        grad_output: (batch, out_features)\n",
        "        Returns: gradient wrt input (batch, in_features)\n",
        "        \"\"\"\n",
        "        x = self.cache[\"x\"]\n",
        "\n",
        "        # Gradients wrt weights and biases\n",
        "        self.grads[\"W\"] = grad_output.T.dot(x)      # (out_features, in_features)\n",
        "        self.grads[\"b\"] = np.sum(grad_output, axis=0, keepdims=True).T  # (out_features, 1)\n",
        "\n",
        "        # Gradient wrt input\n",
        "        grad_input = grad_output.dot(self.params[\"W\"])  # (batch, in_features)\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# ReLU Activation\n",
        "# ==============================\n",
        "class ReLU(Layer):\n",
        "    \"\"\"\n",
        "    ReLU Activation: f(x) = max(0, x)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.trainable = False  # no parameters\n",
        "\n",
        "    def forward(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        # Store mask of where x > 0 for backward\n",
        "        self.cache[\"mask\"] = (x > 0).astype(float)\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        # Pass gradient only where input was positive\n",
        "        return grad_output * self.cache[\"mask\"]\n",
        "\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Dropout2D Implementation\n",
        "# ==============================\n",
        "\n",
        "class Dropout(Layer):\n",
        "    \"\"\"\n",
        "    Standard Dropout (for Dense layers).\n",
        "    Randomly drops individual neurons.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, p: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.trainable = False\n",
        "\n",
        "    def forward(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        if training:\n",
        "            mask = (np.random.rand(*x.shape) > self.p).astype(float)\n",
        "            self.cache[\"mask\"] = mask\n",
        "            return x * mask / (1.0 - self.p)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        mask = self.cache.get(\"mask\", 1.0)\n",
        "        return grad_output * mask / (1.0 - self.p)\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Batch Normalization\n",
        "# ==============================\n",
        "\n",
        "class BatchNorm2D(Layer):\n",
        "    \"\"\"\n",
        "    Batch Normalization for CNNs (per-channel).\n",
        "    Normalizes across batch and spatial dimensions, then scales & shifts.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features: int, eps: float = 1e-5, momentum: float = 0.9):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.params[\"gamma\"] = np.ones((num_features, 1))  # scale\n",
        "        self.params[\"beta\"] = np.zeros((num_features, 1))  # shift\n",
        "\n",
        "        # Running stats (for inference)\n",
        "        self.running_mean = np.zeros((num_features, 1))\n",
        "        self.running_var = np.ones((num_features, 1))\n",
        "\n",
        "    def forward(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        Input: (N, C, H, W)\n",
        "        \"\"\"\n",
        "        N, C, H, W = x.shape\n",
        "\n",
        "        if training:\n",
        "            # Compute mean/var over batch+spatial dims\n",
        "            mean = np.mean(x, axis=(0, 2, 3), keepdims=True)\n",
        "            var = np.var(x, axis=(0, 2, 3), keepdims=True)\n",
        "\n",
        "            # Normalize\n",
        "            x_hat = (x - mean) / np.sqrt(var + self.eps)\n",
        "\n",
        "            # Update running stats\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean.reshape(C, 1)\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var.reshape(C, 1)\n",
        "\n",
        "            # Cache for backward\n",
        "            self.cache = {\"x\": x, \"x_hat\": x_hat, \"mean\": mean, \"var\": var}\n",
        "        else:\n",
        "            # Use running stats for inference\n",
        "            mean = self.running_mean.reshape(1, C, 1, 1)\n",
        "            var = self.running_var.reshape(1, C, 1, 1)\n",
        "            x_hat = (x - mean) / np.sqrt(var + self.eps)\n",
        "\n",
        "        # Scale + shift\n",
        "        out = self.params[\"gamma\"].reshape(1, C, 1, 1) * x_hat + self.params[\"beta\"].reshape(1, C, 1, 1)\n",
        "        return out\n",
        "\n",
        "    def backward(self, grad_output: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Backward pass.\n",
        "        \"\"\"\n",
        "        x = self.cache[\"x\"]\n",
        "        x_hat = self.cache[\"x_hat\"]\n",
        "        mean = self.cache[\"mean\"]\n",
        "        var = self.cache[\"var\"]\n",
        "        N, C, H, W = x.shape\n",
        "        m = N * H * W  # number of elements per channel\n",
        "\n",
        "        # Grad wrt gamma and beta\n",
        "        self.grads[\"gamma\"] = np.sum(grad_output * x_hat, axis=(0, 2, 3), keepdims=True).reshape(C, 1)\n",
        "        self.grads[\"beta\"] = np.sum(grad_output, axis=(0, 2, 3), keepdims=True).reshape(C, 1)\n",
        "\n",
        "        # Grad wrt input\n",
        "        dx_hat = grad_output * self.params[\"gamma\"].reshape(1, C, 1, 1)\n",
        "        dvar = np.sum(dx_hat * (x - mean) * -0.5 * (var + self.eps) ** (-1.5), axis=(0, 2, 3), keepdims=True)\n",
        "        dmean = np.sum(dx_hat * -1 / np.sqrt(var + self.eps), axis=(0, 2, 3), keepdims=True) + \\\n",
        "                dvar * np.sum(-2 * (x - mean), axis=(0, 2, 3), keepdims=True) / m\n",
        "\n",
        "        dx = dx_hat / np.sqrt(var + self.eps) + dvar * 2 * (x - mean) / m + dmean / m\n",
        "        return dx\n"
      ],
      "metadata": {
        "id": "qOI-N-bW9330"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: CNN Architectures"
      ],
      "metadata": {
        "id": "l9O7ikdGbE5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s assemble the LeNet-5 architecture using the building blocks we already coded (Conv2D, ReLU, MaxPool2D, Flatten, Dense)"
      ],
      "metadata": {
        "id": "LnKLAy8A_HdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet-5 Architecture Reminder (from your README)"
      ],
      "metadata": {
        "id": "wuSypWB2_VF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Input (32x32x3) →\n",
        "Conv(6, 5x5) → ReLU → MaxPool(2x2) →\n",
        "Conv(16, 5x5) → ReLU → MaxPool(2x2) →\n",
        "Flatten → FC(120) → ReLU → FC(84) → ReLU → FC(10)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ZKJLCyaR_LRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation"
      ],
      "metadata": {
        "id": "TU7N1p5j_uSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5:\n",
        "    \"\"\"\n",
        "    LeNet-5 with BatchNorm added\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes: int = 10):\n",
        "        self.layers = [\n",
        "            Conv2D(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=\"valid\"),\n",
        "            BatchNorm2D(num_features=6),\n",
        "            ReLU(),\n",
        "            MaxPool2D(pool_size=2, stride=2),\n",
        "\n",
        "            Conv2D(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=\"valid\"),\n",
        "            BatchNorm2D(num_features=16),\n",
        "            ReLU(),\n",
        "            MaxPool2D(pool_size=2, stride=2),\n",
        "\n",
        "            Flatten(),\n",
        "\n",
        "            Dense(in_features=16*5*5, out_features=120),\n",
        "            ReLU(),\n",
        "\n",
        "            Dense(in_features=120, out_features=84),\n",
        "            ReLU(),\n",
        "\n",
        "            Dense(in_features=84, out_features=num_classes)\n",
        "        ]\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x, training)\n",
        "        return x\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        for layer in reversed(self.layers):\n",
        "            grad_output = layer.backward(grad_output)\n",
        "        return grad_output\n",
        "\n",
        "    def get_params(self):\n",
        "        params = {}\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if layer.trainable:\n",
        "                for k, v in layer.params.items():\n",
        "                    params[f\"layer{idx}_{k}\"] = v\n",
        "        return params\n",
        "\n",
        "    def get_grads(self):\n",
        "        grads = {}\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if layer.trainable:\n",
        "                for k, v in layer.grads.items():\n",
        "                    grads[f\"layer{idx}_{k}\"] = v\n",
        "        return grads\n"
      ],
      "metadata": {
        "id": "AOxNOWDy97QD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "model = LeNet5(num_classes=10)\n",
        "X = np.random.randn(2, 3, 32, 32)  # dummy batch of 2 RGB images\n",
        "out = model.forward(X)\n",
        "print(\"Output shape:\", out.shape)  # (2, 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxhddA23_8Ir",
        "outputId": "b68bd16b-e8ec-4a77-d0f9-cc757a78702e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1210623038.py:132: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  out[n, f, i, j] = np.sum(patch * W[f]) + b[f]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (2, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have Conv2D, MaxPool2D, Flatten, Dense, ReLU working, let’s build the **Mini-VGG architecture** described in your README."
      ],
      "metadata": {
        "id": "PqMKRh46BL9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Input (32x32x3) →\n",
        "Conv(32, 3x3) → ReLU → Conv(32, 3x3) → ReLU → MaxPool(2x2) →\n",
        "Conv(64, 3x3) → ReLU → Conv(64, 3x3) → ReLU → MaxPool(2x2) →\n",
        "Conv(128, 3x3) → ReLU → Conv(128, 3x3) → ReLU → MaxPool(2x2) →\n",
        "Flatten → FC(256) → ReLU → Dropout(0.5) → FC(10)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "LDooVmlOBPqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Simplified VGG-style network for CIFAR-10 (input: 32x32x3, output: 10 classes)\n",
        "\n",
        "class MiniVGG:\n",
        "\n",
        "    def __init__(self, num_classes: int = 10):\n",
        "        self.layers = [\n",
        "            # Block 1\n",
        "            Conv2D(3, 32, kernel_size=3, stride=1, padding=\"same\"),\n",
        "            BatchNorm2D(32),\n",
        "            ReLU(),\n",
        "            Conv2D(32, 32, kernel_size=3, stride=1, padding=\"same\"),\n",
        "            BatchNorm2D(32),\n",
        "            ReLU(),\n",
        "            MaxPool2D(pool_size=2, stride=2),\n",
        "\n",
        "            # Block 2\n",
        "            Conv2D(32, 64, kernel_size=3, stride=1, padding=\"same\"),\n",
        "            BatchNorm2D(64),\n",
        "            ReLU(),\n",
        "            Conv2D(64, 64, kernel_size=3, stride=1, padding=\"same\"),\n",
        "            BatchNorm2D(64),\n",
        "            ReLU(),\n",
        "            MaxPool2D(pool_size=2, stride=2),\n",
        "\n",
        "            # Block 3\n",
        "            Conv2D(64, 128, kernel_size=3, stride=1, padding=\"same\"),\n",
        "            BatchNorm2D(128),\n",
        "            ReLU(),\n",
        "            Conv2D(128, 128, kernel_size=3, stride=1, padding=\"same\"),\n",
        "            BatchNorm2D(128),\n",
        "            ReLU(),\n",
        "            MaxPool2D(pool_size=2, stride=2),\n",
        "\n",
        "            # Fully connected\n",
        "            Flatten(),\n",
        "            Dense(128*4*4, 256),\n",
        "            ReLU(),\n",
        "            Dropout(p=0.5),\n",
        "            Dense(256, num_classes)\n",
        "        ]\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x, training)\n",
        "        return x\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        for layer in reversed(self.layers):\n",
        "            grad_output = layer.backward(grad_output)\n",
        "        return grad_output\n",
        "\n",
        "    def get_params(self):\n",
        "        params = {}\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if layer.trainable:\n",
        "                for k, v in layer.params.items():\n",
        "                    params[f\"layer{idx}_{k}\"] = v\n",
        "        return params\n",
        "\n",
        "    def get_grads(self):\n",
        "        grads = {}\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if layer.trainable:\n",
        "                for k, v in layer.grads.items():\n",
        "                    grads[f\"layer{idx}_{k}\"] = v\n",
        "        return grads\n"
      ],
      "metadata": {
        "id": "vkm7cLxKQ0Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "\n",
        "Padding = \"same\" ensures spatial dimensions stay the same after convolution.\n",
        "\n",
        "After 3 max poolings (stride=2):\n",
        "\n",
        "  1. Input 32×32 → 16×16 → 8×8 → 4×4\n",
        "\n",
        "  2. With 128 channels → 128 × 4 × 4 = 2048 features.\n",
        "\n",
        "Dropout2D(p=0.5) randomly drops half of the feature maps during training to reduce overfitting."
      ],
      "metadata": {
        "id": "4GOZwN11B8n6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Quick Test"
      ],
      "metadata": {
        "id": "RPPvjPSPCQKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Mini-VGG model\n",
        "model = MiniVGG(num_classes=10)\n",
        "# Dummy input: batch of 2 RGB images (32x32)\n",
        "X = np.random.randn(2, 3, 32, 32)\n",
        "# Forward pass\n",
        "out = model.forward(X, training=True)\n",
        "print(\"Output shape:\", out.shape)  # should be (2, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tv9ZWzKBXUr",
        "outputId": "215aa0eb-5ecb-4302-a0bc-d8979b254866"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1210623038.py:132: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  out[n, f, i, j] = np.sum(patch * W[f]) + b[f]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (2, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: CIFAR-10 Classification"
      ],
      "metadata": {
        "id": "kFH37N0_bovJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that LeNet-5 and Mini-VGG are built, the next step is to train them. For that, we need three key pieces:\n",
        "\n",
        "  1. Loss function → Cross-Entropy (commonly used for classification).\n",
        "\n",
        "  2. Optimizer → Stochastic Gradient Descent (SGD).\n",
        "\n",
        "  3. Training Loop → to iterate over batches, forward → loss → backward → update."
      ],
      "metadata": {
        "id": "h5FG7aXwDxLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Cross-Entropy Loss (with Softmax)"
      ],
      "metadata": {
        "id": "56cYzqH0D-yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossEntropyLoss:\n",
        "    \"\"\"\n",
        "    Cross-Entropy Loss with Softmax.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.cache = {}\n",
        "\n",
        "    def forward(self, logits: np.ndarray, labels: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        logits: raw output from model (batch, num_classes)\n",
        "        labels: true class indices (batch,)\n",
        "        \"\"\"\n",
        "        # Shift logits for numerical stability\n",
        "        shifted_logits = logits - np.max(logits, axis=1, keepdims=True)\n",
        "\n",
        "        # Compute softmax probabilities\n",
        "        exp_logits = np.exp(shifted_logits)\n",
        "        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "        # Negative log likelihood loss\n",
        "        N = logits.shape[0]\n",
        "        correct_logprobs = -np.log(probs[np.arange(N), labels])\n",
        "        loss = np.mean(correct_logprobs)\n",
        "\n",
        "        # Save probs + labels for backward\n",
        "        self.cache[\"probs\"] = probs\n",
        "        self.cache[\"labels\"] = labels\n",
        "        return loss\n",
        "\n",
        "    def backward(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Gradient of loss wrt logits.\n",
        "        \"\"\"\n",
        "        probs = self.cache[\"probs\"]\n",
        "        labels = self.cache[\"labels\"]\n",
        "        N = probs.shape[0]\n",
        "\n",
        "        grad_logits = probs.copy()\n",
        "        grad_logits[np.arange(N), labels] -= 1\n",
        "        grad_logits /= N\n",
        "        return grad_logits\n"
      ],
      "metadata": {
        "id": "LnhcV2FGD_Vq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Optimizer (SGD)"
      ],
      "metadata": {
        "id": "hqs-3uKsEO5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    \"\"\"\n",
        "    Stochastic Gradient Descent optimizer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, lr=0.01):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Update model parameters using stored gradients.\n",
        "        \"\"\"\n",
        "        params = self.model.get_params()\n",
        "        grads = self.model.get_grads()\n",
        "\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]\n"
      ],
      "metadata": {
        "id": "cxbpncgIERKS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Training Loop"
      ],
      "metadata": {
        "id": "VmhSBR2AET-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(preds, labels):\n",
        "    \"\"\"Compute accuracy\"\"\"\n",
        "    return np.mean(np.argmax(preds, axis=1) == labels)\n",
        "\n",
        "\n",
        "def train(model, X_train, y_train, X_val, y_val, epochs=5, batch_size=64, lr=0.01):\n",
        "    \"\"\"\n",
        "    Simple training loop for CNNs.\n",
        "    \"\"\"\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "    optimizer = SGD(model, lr=lr)\n",
        "\n",
        "    num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss, epoch_acc = 0, 0\n",
        "\n",
        "        # Shuffle training data\n",
        "        indices = np.arange(X_train.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            batch_idx = indices[i*batch_size:(i+1)*batch_size]\n",
        "            X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model.forward(X_batch, training=True)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn.forward(logits, y_batch)\n",
        "\n",
        "            # Backward pass\n",
        "            grad_logits = loss_fn.backward()\n",
        "            model.backward(grad_logits)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track batch accuracy\n",
        "            acc = accuracy(logits, y_batch)\n",
        "            epoch_loss += loss\n",
        "            epoch_acc += acc\n",
        "\n",
        "        # Average over batches\n",
        "        epoch_loss /= num_batches\n",
        "        epoch_acc /= num_batches\n",
        "\n",
        "        # Validation\n",
        "        val_logits = model.forward(X_val, training=False)\n",
        "        val_loss = loss_fn.forward(val_logits, y_val)\n",
        "        val_acc = accuracy(val_logits, y_val)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "gz3JosRMEWQ6"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Usage**"
      ],
      "metadata": {
        "id": "SMnTIg46Efn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy CIFAR-like data (small for demo)\n",
        "X_train = np.random.randn(100, 3, 32, 32)\n",
        "y_train = np.random.randint(0, 10, 100)\n",
        "\n",
        "X_val = np.random.randn(20, 3, 32, 32)\n",
        "y_val = np.random.randint(0, 10, 20)\n",
        "\n",
        "# Pick a model (LeNet5 or MiniVGG)\n",
        "model = LeNet5(num_classes=10)\n",
        "# model = MiniVGG(num_classes=10)\n",
        "\n",
        "# Train\n",
        "train(model, X_train, y_train, X_val, y_val, epochs=3, batch_size=16, lr=0.01)\n"
      ],
      "metadata": {
        "id": "3clgPKB1Ecoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "\n",
        "  CrossEntropyLoss: combines softmax + negative log likelihood in one.\n",
        "\n",
        "  SGD: updates parameters after each batch.\n",
        "\n",
        "  Training loop:\n",
        "\n",
        "    Forward pass → predictions\n",
        "\n",
        "    Compute loss\n",
        "\n",
        "    Backward pass → gradients\n",
        "\n",
        "    Optimizer step → update weights\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_IOT5r_OElET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s plug in real CIFAR-10 data instead of dummy arrays, so you can actually train LeNet-5 or Mini-VGG.\n",
        "\n",
        "Since we’re coding this from scratch with NumPy only, we’ll use Keras dataset loader (very convenient, no PyTorch/TensorFlow training needed)."
      ],
      "metadata": {
        "id": "IGtmz09uG4yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: CIFAR-10 Loader"
      ],
      "metadata": {
        "id": "5m9rzbBjG8mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cifar10(normalize=True):\n",
        "    \"\"\"\n",
        "    Load CIFAR-10 dataset and return NumPy arrays.\n",
        "    Shape:\n",
        "      X_train: (50000, 3, 32, 32)\n",
        "      y_train: (50000,)\n",
        "      X_test:  (10000, 3, 32, 32)\n",
        "      y_test:  (10000,)\n",
        "    \"\"\"\n",
        "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "    # Convert from (N, 32, 32, 3) → (N, 3, 32, 32)\n",
        "    X_train = X_train.transpose(0, 3, 1, 2).astype(np.float32)\n",
        "    X_test = X_test.transpose(0, 3, 1, 2).astype(np.float32)\n",
        "\n",
        "    # Flatten labels\n",
        "    y_train = y_train.flatten()\n",
        "    y_test = y_test.flatten()\n",
        "\n",
        "    # Normalize to [0,1] if required\n",
        "    if normalize:\n",
        "        X_train /= 255.0\n",
        "        X_test /= 255.0\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n"
      ],
      "metadata": {
        "id": "uuxZ_9qKEkLT"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Train on CIFAR-10"
      ],
      "metadata": {
        "id": "yt0hewB5HD5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "X_train, y_train, X_test, y_test = load_cifar10()\n",
        "\n",
        "# Split validation set\n",
        "X_val, y_val = X_train[:5000], y_train[:5000]\n",
        "X_train, y_train = X_train[5000:], y_train[5000:]\n",
        "\n",
        "print(\"Train set:\", X_train.shape, y_train.shape)\n",
        "print(\"Val set:\", X_val.shape, y_val.shape)\n",
        "print(\"Test set:\", X_test.shape, y_test.shape)\n",
        "\n",
        "# Choose a model\n",
        "model = LeNet5(num_classes=10)\n",
        "# model = MiniVGG(num_classes=10)   # <- More powerful, but slower\n",
        "\n",
        "# Train\n",
        "train(model, X_train, y_train, X_val, y_val,\n",
        "      epochs=5, batch_size=64, lr=0.01)\n",
        "\n",
        "# Final evaluation on test set\n",
        "loss_fn = CrossEntropyLoss()\n",
        "logits = model.forward(X_test, training=False)\n",
        "test_loss = loss_fn.forward(logits, y_test)\n",
        "test_acc = accuracy(logits, y_test)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2zsi2gVHFTD",
        "outputId": "9595cd27-b062-40ba-adc1-36bf1be8a453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 0us/step\n",
            "Train set: (45000, 3, 32, 32) (45000,)\n",
            "Val set: (5000, 3, 32, 32) (5000,)\n",
            "Test set: (10000, 3, 32, 32) (10000,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1210623038.py:132: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  out[n, f, i, j] = np.sum(patch * W[f]) + b[f]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes**\n",
        "\n",
        "    Normalization → Dividing by 255 scales pixels from [0,255] → [0,1], which makes training stable.\n",
        "\n",
        "    Transpose → Keras loads images as (N, H, W, C), but our network expects (N, C, H, W).\n",
        "\n",
        "    Split validation → We take first 5000 images from training set as validation.\n",
        "\n",
        "    Training → You’ll see epoch-wise loss and accuracy for both training and validation.\n",
        "\n",
        "    Test set → Evaluate model after training."
      ],
      "metadata": {
        "id": "wHc92WgdHWSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ Note: Since our implementation is pure NumPy (no GPU acceleration), training on full CIFAR-10 will be slow.</br>\n",
        "👉 For testing, you might want to use smaller subsets first, e.g., X_train[:2000], y_train[:2000]."
      ],
      "metadata": {
        "id": "82jufQHIHvQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s add data augmentation (as required in your assignment). This helps prevent overfitting and improves accuracy, especially for Mini-VGG.\n",
        "\n",
        "We’ll implement:\n",
        "\n",
        "    Random horizontal flip\n",
        "\n",
        "    Random shift (translation)\n",
        "\n",
        "    Random rotation"
      ],
      "metadata": {
        "id": "2L4QeK1vLfWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DataAugmentation:\n",
        "    \"\"\"\n",
        "    Data augmentation for CIFAR-10 images.\n",
        "    Works on NumPy arrays shaped (batch, C, H, W).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, horizontal_flip=True, rotation_range=15, shift_range=0.1):\n",
        "        self.horizontal_flip = horizontal_flip\n",
        "        self.rotation_range = rotation_range  # degrees\n",
        "        self.shift_range = shift_range        # fraction of image size\n",
        "\n",
        "    def augment_batch(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Apply random augmentations to a batch of images.\n",
        "        \"\"\"\n",
        "        X_aug = np.empty_like(X)\n",
        "        batch_size, C, H, W = X.shape\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            img = X[i].transpose(1, 2, 0)  # (H, W, C) for easier manipulation\n",
        "\n",
        "            # Random horizontal flip\n",
        "            if self.horizontal_flip and np.random.rand() < 0.5:\n",
        "                img = np.fliplr(img)\n",
        "\n",
        "            # Random rotation\n",
        "            if self.rotation_range > 0:\n",
        "                angle = np.random.uniform(-self.rotation_range, self.rotation_range)\n",
        "                img = rotate(img, angle, reshape=False, mode=\"reflect\")\n",
        "\n",
        "            # Random shift (translation)\n",
        "            if self.shift_range > 0:\n",
        "                shift_h = int(np.random.uniform(-self.shift_range, self.shift_range) * H)\n",
        "                shift_w = int(np.random.uniform(-self.shift_range, self.shift_range) * W)\n",
        "\n",
        "                # Create empty canvas\n",
        "                shifted = np.zeros_like(img)\n",
        "                h_start = max(0, shift_h)\n",
        "                h_end = H + min(0, shift_h)\n",
        "                w_start = max(0, shift_w)\n",
        "                w_end = W + min(0, shift_w)\n",
        "\n",
        "                shifted[h_start:h_end, w_start:w_end] = img[\n",
        "                    max(0, -shift_h):H - max(0, shift_h),\n",
        "                    max(0, -shift_w):W - max(0, shift_w)\n",
        "                ]\n",
        "                img = shifted\n",
        "\n",
        "            # Store back\n",
        "            X_aug[i] = img.transpose(2, 0, 1)  # back to (C, H, W)\n",
        "\n",
        "        return X_aug\n"
      ],
      "metadata": {
        "id": "MFmqfxe7He97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📝 Updating Training Loop with Augmentation\n",
        "\n",
        "We integrate augmentation in the train loop (only for training batches, not validation/test):"
      ],
      "metadata": {
        "id": "4SXTSt1YMCKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, X_train, y_train, X_val, y_val,\n",
        "          epochs=5, batch_size=64, lr=0.01, augment=False):\n",
        "    \"\"\"\n",
        "    Simple training loop with optional data augmentation.\n",
        "    \"\"\"\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "    optimizer = SGD(model, lr=lr)\n",
        "    augmenter = DataAugmentation() if augment else None\n",
        "\n",
        "    num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss, epoch_acc = 0, 0\n",
        "\n",
        "        # Shuffle training data\n",
        "        indices = np.arange(X_train.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            batch_idx = indices[i*batch_size:(i+1)*batch_size]\n",
        "            X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
        "\n",
        "            # Apply augmentation if enabled\n",
        "            if augment:\n",
        "                X_batch = augmenter.augment_batch(X_batch)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model.forward(X_batch, training=True)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn.forward(logits, y_batch)\n",
        "\n",
        "            # Backward pass\n",
        "            grad_logits = loss_fn.backward()\n",
        "            model.backward(grad_logits)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track batch accuracy\n",
        "            acc = accuracy(logits, y_batch)\n",
        "            epoch_loss += loss\n",
        "            epoch_acc += acc\n",
        "\n",
        "        # Average over batches\n",
        "        epoch_loss /= num_batches\n",
        "        epoch_acc /= num_batches\n",
        "\n",
        "        # Validation\n",
        "        val_logits = model.forward(X_val, training=False)\n",
        "        val_loss = loss_fn.forward(val_logits, y_val)\n",
        "        val_acc = accuracy(val_logits, y_val)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "3hcJyyEnL-3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Example Usage with Augmentation"
      ],
      "metadata": {
        "id": "2FNOEsjxMIx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CIFAR-10\n",
        "X_train, y_train, X_test, y_test = load_cifar10()\n",
        "\n",
        "# Create validation split\n",
        "X_val, y_val = X_train[:500], y_train[:500]\n",
        "X_train, y_train = X_train[500:2500], y_train[500:2500]  # use only 2000 samples for speed\n",
        "\n",
        "print(\"Train set:\", X_train.shape, y_train.shape)\n",
        "print(\"Val set:\", X_val.shape, y_val.shape)\n",
        "\n",
        "# Create model\n",
        "model = MiniVGG(num_classes=10)\n",
        "\n",
        "# Train (with augmentation enabled)\n",
        "train(model,\n",
        "      X_train, y_train,\n",
        "      X_val, y_val,\n",
        "      epochs=3, batch_size=32, lr=0.01,\n",
        "      augment=True)\n",
        "\n",
        "# Quick test evaluation on 200 test images\n",
        "loss_fn = CrossEntropyLoss()\n",
        "logits = model.forward(X_test[:200], training=False)\n",
        "test_loss = loss_fn.forward(logits, y_test[:200])\n",
        "test_acc = accuracy(logits, y_test[:200])\n",
        "\n",
        "print(f\"Subset Test Loss: {test_loss:.4f}, Subset Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "Kn3UJlVoPUHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "\n",
        "    Flip: makes network invariant to left-right orientation.\n",
        "\n",
        "    Rotation: helps recognize rotated objects.\n",
        "\n",
        "    Shift: teaches robustness to translation.\n",
        "\n",
        "    Only applied during training; validation/test sets are left untouched."
      ],
      "metadata": {
        "id": "_OXGRFkbMqR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ Training on full CIFAR-10 (50,000 images) with NumPy CNN will be very slow.<br>\n",
        "👉 For testing, use smaller subsets first (X_train[:2000], etc.), then scale up."
      ],
      "metadata": {
        "id": "isMZ_tykMxt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s add a visualization helper so you can see how your CIFAR-10 images look before and after augmentation."
      ],
      "metadata": {
        "id": "sesU2COqM8qC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📝 Visualization Function"
      ],
      "metadata": {
        "id": "XcekGX_oNAea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def show_augmentation(X_batch, augmenter, n=5):\n",
        "    \"\"\"\n",
        "    Visualize original vs augmented images side by side.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_batch : np.ndarray\n",
        "        A batch of images (N, C, H, W)\n",
        "    augmenter : DataAugmentation\n",
        "        Augmentation object\n",
        "    n : int\n",
        "        Number of samples to display\n",
        "    \"\"\"\n",
        "    # Pick first n images\n",
        "    X_orig = X_batch[:n]\n",
        "    X_aug = augmenter.augment_batch(X_orig.copy())\n",
        "\n",
        "    plt.figure(figsize=(2*n, 4))\n",
        "\n",
        "    for i in range(n):\n",
        "        # Original\n",
        "        plt.subplot(2, n, i+1)\n",
        "        img = X_orig[i].transpose(1, 2, 0)  # (H,W,C)\n",
        "        plt.imshow(np.clip(img, 0, 1))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Original\")\n",
        "\n",
        "        # Augmented\n",
        "        plt.subplot(2, n, n+i+1)\n",
        "        img_aug = X_aug[i].transpose(1, 2, 0)\n",
        "        plt.imshow(np.clip(img_aug, 0, 1))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Augmented\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "ncwDUhp1Mtr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Example Usage"
      ],
      "metadata": {
        "id": "8h2BRZGwNJkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a small batch of CIFAR-10\n",
        "X_train, y_train, X_test, y_test = load_cifar10()\n",
        "augmenter = DataAugmentation(horizontal_flip=True, rotation_range=20, shift_range=0.2)\n",
        "\n",
        "# Show first 5 images before/after augmentation\n",
        "show_augmentation(X_train[:5], augmenter, n=5)"
      ],
      "metadata": {
        "id": "FepEfAIuNKFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate Scheduling\n",
        "👉 Learning rate scheduling means changing the learning rate during training to improve convergence.\n",
        "\n",
        "Typical policies:\n",
        "\n",
        "    Step decay: reduce lr every few epochs (e.g., lr *= 0.1 every 10 epochs).\n",
        "\n",
        "    Exponential decay: lr = lr * exp(-decay * epoch).\n",
        "\n",
        "    Plateau decay: reduce lr when validation accuracy stops improving."
      ],
      "metadata": {
        "id": "AJYqcdNIRwtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code: Learning Rate Scheduler"
      ],
      "metadata": {
        "id": "YwWgpeiHSLMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, X_train, y_train, X_val, y_val,\n",
        "          epochs=5, batch_size=64, lr=0.01, augment=False,\n",
        "          scheduler=None):\n",
        "    \"\"\"\n",
        "    Training loop with optional augmentation and LR scheduler.\n",
        "    \"\"\"\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "    optimizer = SGD(model, lr=lr)\n",
        "    augmenter = DataAugmentation() if augment else None\n",
        "\n",
        "    num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss, epoch_acc = 0, 0\n",
        "\n",
        "        # Shuffle training data\n",
        "        indices = np.arange(X_train.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            batch_idx = indices[i*batch_size:(i+1)*batch_size]\n",
        "            X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
        "\n",
        "            if augment:\n",
        "                X_batch = augmenter.augment_batch(X_batch)\n",
        "\n",
        "            # Forward\n",
        "            logits = model.forward(X_batch, training=True)\n",
        "            loss = loss_fn.forward(logits, y_batch)\n",
        "\n",
        "            # Backward\n",
        "            grad_logits = loss_fn.backward()\n",
        "            model.backward(grad_logits)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            acc = accuracy(logits, y_batch)\n",
        "            epoch_loss += loss\n",
        "            epoch_acc += acc\n",
        "\n",
        "        # Average over batches\n",
        "        epoch_loss /= num_batches\n",
        "        epoch_acc /= num_batches\n",
        "\n",
        "        # Validation\n",
        "        val_logits = model.forward(X_val, training=False)\n",
        "        val_loss = loss_fn.forward(val_logits, y_val)\n",
        "        val_acc = accuracy(val_logits, y_val)\n",
        "\n",
        "        # Apply LR scheduling\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "nkJ4Ox2UNMlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔧 Update Training Loop"
      ],
      "metadata": {
        "id": "h5X_k5fLSQfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, X_train, y_train, X_val, y_val,\n",
        "          epochs=5, batch_size=64, lr=0.01, augment=False,\n",
        "          scheduler=None):\n",
        "    \"\"\"\n",
        "    Training loop with optional augmentation and LR scheduler.\n",
        "    \"\"\"\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "    optimizer = SGD(model, lr=lr)\n",
        "    augmenter = DataAugmentation() if augment else None\n",
        "\n",
        "    num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss, epoch_acc = 0, 0\n",
        "\n",
        "        # Shuffle training data\n",
        "        indices = np.arange(X_train.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            batch_idx = indices[i*batch_size:(i+1)*batch_size]\n",
        "            X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
        "\n",
        "            if augment:\n",
        "                X_batch = augmenter.augment_batch(X_batch)\n",
        "\n",
        "            # Forward\n",
        "            logits = model.forward(X_batch, training=True)\n",
        "            loss = loss_fn.forward(logits, y_batch)\n",
        "\n",
        "            # Backward\n",
        "            grad_logits = loss_fn.backward()\n",
        "            model.backward(grad_logits)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "\n",
        "            acc = accuracy(logits, y_batch)\n",
        "            epoch_loss += loss\n",
        "            epoch_acc += acc\n",
        "\n",
        "        # Average over batches\n",
        "        epoch_loss /= num_batches\n",
        "        epoch_acc /= num_batches\n",
        "\n",
        "        # Validation\n",
        "        val_logits = model.forward(X_val, training=False)\n",
        "        val_loss = loss_fn.forward(val_logits, y_val)\n",
        "        val_acc = accuracy(val_logits, y_val)\n",
        "\n",
        "        # Apply LR scheduling\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(epoch)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "r0Gx9UFXSRAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Example Usage"
      ],
      "metadata": {
        "id": "q1h_9k0uSXaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training on a small subset with LR scheduling\n",
        "model = MiniVGG(num_classes=10)\n",
        "scheduler = LRScheduler(optimizer=SGD(model, lr=0.01), step_size=2, gamma=0.5)\n",
        "\n",
        "train(model,\n",
        "      X_train[:2000], y_train[:2000],\n",
        "      X_val[:500], y_val[:500],\n",
        "      epochs=6, batch_size=64, lr=0.01,\n",
        "      augment=True,\n",
        "      scheduler=scheduler)\n"
      ],
      "metadata": {
        "id": "XOD06PwYSX_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📝 Confusion Matrix + Per-Class Accuracy"
      ],
      "metadata": {
        "id": "Xnm5sPIkSpCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def confusion_matrix(preds: np.ndarray, labels: np.ndarray, num_classes=10):\n",
        "    \"\"\"\n",
        "    Compute confusion matrix.\n",
        "\n",
        "    preds: model predictions (N,) or logits (N, num_classes)\n",
        "    labels: true labels (N,)\n",
        "    \"\"\"\n",
        "    if preds.ndim > 1:  # if logits, take argmax\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
        "    for t, p in zip(labels, preds):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n",
        "\n",
        "def per_class_accuracy(cm: np.ndarray):\n",
        "    \"\"\"\n",
        "    Compute per-class accuracy from confusion matrix.\n",
        "    \"\"\"\n",
        "    accs = cm.diagonal() / cm.sum(axis=1, where=(cm.sum(axis=1)!=0))\n",
        "    return accs\n",
        "\n",
        "def plot_confusion_matrix(cm: np.ndarray, class_names=None, title=\"Confusion Matrix\"):\n",
        "    \"\"\"\n",
        "    Display confusion matrix using heatmap.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "m6EOnRDhSnNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Example Usage After Training"
      ],
      "metadata": {
        "id": "M20kBZp3Su_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume model is trained already\n",
        "\n",
        "# Forward pass on test set (smaller subset for speed)\n",
        "logits = model.forward(X_test[:1000], training=False)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(logits, y_test[:1000], num_classes=10)\n",
        "\n",
        "# Per-class accuracy\n",
        "accs = per_class_accuracy(cm)\n",
        "for i, acc in enumerate(accs):\n",
        "    print(f\"Class {i}: {acc:.2f}\")\n",
        "\n",
        "# Plot matrix\n",
        "plot_confusion_matrix(cm, class_names=[str(i) for i in range(10)])\n"
      ],
      "metadata": {
        "id": "SNp4h4B4St56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "\n",
        "    Confusion matrix → shows how many times each class was predicted correctly (diagonal) vs. misclassified (off-diagonal).\n",
        "\n",
        "    Per-class accuracy → fraction of correct predictions for each class.\n",
        "\n",
        "    Plotting → heatmap visualization makes it easy to spot which classes the network struggles with."
      ],
      "metadata": {
        "id": "gSbEM-sTS26A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Feature Visualization."
      ],
      "metadata": {
        "id": "T8MwNeURTA0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s move to README Part 4: Feature Visualization.\n",
        "\n",
        "The README requires two things:\n",
        "\n",
        "    Filter Visualization → show the learned filters of the first Conv layer.\n",
        "\n",
        "    Feature Map Visualization → show activations (feature maps) inside the network for a sample image."
      ],
      "metadata": {
        "id": "g51K7NdfTFHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📝 Filter Visualization"
      ],
      "metadata": {
        "id": "iat0kIGTTL55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_filters(conv_layer, save_path=\"filters.png\"):\n",
        "    \"\"\"\n",
        "    Visualize learned filters of the first Conv2D layer.\n",
        "\n",
        "    conv_layer: Conv2D object\n",
        "    \"\"\"\n",
        "    W = conv_layer.params[\"W\"]  # shape (out_channels, in_channels, kh, kw)\n",
        "    num_filters = W.shape[0]\n",
        "    num_channels = W.shape[1]\n",
        "\n",
        "    # Normalize filters to 0-1 for display\n",
        "    W_min, W_max = W.min(), W.max()\n",
        "    W = (W - W_min) / (W_max - W_min + 1e-8)\n",
        "\n",
        "    # Plot each filter\n",
        "    cols = 8\n",
        "    rows = int(np.ceil(num_filters / cols))\n",
        "    plt.figure(figsize=(cols, rows))\n",
        "\n",
        "    for i in range(num_filters):\n",
        "        f = W[i]\n",
        "        if num_channels == 3:  # RGB filters\n",
        "            f_img = np.transpose(f, (1,2,0))\n",
        "        else:  # grayscale\n",
        "            f_img = f[0]\n",
        "        plt.subplot(rows, cols, i+1)\n",
        "        plt.imshow(f_img, cmap=\"viridis\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(\"Learned Filters\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "g_CJ3jcxS2fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📝 Feature Map Visualization"
      ],
      "metadata": {
        "id": "5qUUC9MkTPcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_feature_maps(model, X_sample, layer_indices, save_path=\"feature_maps.png\"):\n",
        "    \"\"\"\n",
        "    Visualize feature maps at specified layers.\n",
        "\n",
        "    model: LeNet5 or MiniVGG instance\n",
        "    X_sample: single image (C, H, W)\n",
        "    layer_indices: list of indices of layers to visualize\n",
        "    \"\"\"\n",
        "    x = X_sample[np.newaxis, ...]  # add batch dim\n",
        "    activations = []\n",
        "\n",
        "    # Forward pass while storing intermediate outputs\n",
        "    for idx, layer in enumerate(model.layers):\n",
        "        x = layer.forward(x, training=False)\n",
        "        if idx in layer_indices:\n",
        "            activations.append((idx, x.copy()))\n",
        "\n",
        "    # Plot\n",
        "    for idx, act in activations:\n",
        "        num_maps = act.shape[1]\n",
        "        cols = 8\n",
        "        rows = int(np.ceil(num_maps / cols))\n",
        "        plt.figure(figsize=(cols, rows))\n",
        "        for i in range(num_maps):\n",
        "            plt.subplot(rows, cols, i+1)\n",
        "            plt.imshow(act[0, i], cmap=\"gray\")\n",
        "            plt.axis(\"off\")\n",
        "        plt.suptitle(f\"Feature Maps at Layer {idx}\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"feature_maps_layer{idx}.png\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "KIZCh1KOSaJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Example Usage"
      ],
      "metadata": {
        "id": "-kgkzML_TUjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After training your model\n",
        "# Show filters of first Conv layer\n",
        "visualize_filters(model.layers[0])\n",
        "\n",
        "# Pick one test image\n",
        "X_sample = X_test[0]\n",
        "\n",
        "# Show feature maps at some Conv layers\n",
        "visualize_feature_maps(model, X_sample, layer_indices=[0, 3, 7])\n"
      ],
      "metadata": {
        "id": "AdyCXQ5iTWi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes**\n",
        "\n",
        "    Filters (weights of conv kernels) in early layers often look like edge detectors or color blobs.\n",
        "\n",
        "    Feature maps show what each channel is “looking for” in the image (edges, textures, shapes).\n",
        "\n",
        "    As you go deeper → features become more abstract (object parts, high-level patterns)."
      ],
      "metadata": {
        "id": "-Dfj200YTaYB"
      }
    }
  ]
}